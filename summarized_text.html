<html><head><style>body {background-color: #2B2B2B; color: #F0F0F0; font-family: Arial, sans-serif;}</style></head><body>• Introduction<br>&emsp;&emsp;• In this section, Sam Altman talks about the early days of OpenAI and how they were mocked for their goal of building AGI. He also discusses the importance of conversations about AI and its power.<br>&emsp;&emsp;• The Early Days of OpenAI<br>&emsp;&emsp;• OpenAI was mocked when it announced its goal to work on AGI in 2015.<br>&emsp;&emsp;• An eminent AI scientist at a large industrial AI lab even DM'ed individual reporters to discredit them.<br>&emsp;&emsp;• OpenAI and DeepMind were brave enough to talk about AGI in the face of mockery.<br>&emsp;&emsp;• Importance of Conversations About AI<br>&emsp;&emsp;• Conversations about AI are not just technical but also about power, companies, institutions, political systems, and human nature.<br>&emsp;&emsp;• These conversations are important now because we stand on the precipice of fundamental societal transformation where superintelligence in AI systems begins to pale in comparison to the collective intelligence of the human species.<br>&emsp;&emsp;• 01:05 Possibilities and Dangers of AI<br>&emsp;&emsp;• In this section, Sam Altman talks about the possibilities and dangers of AI. He discusses how it can empower humans but also destroy human civilization if not aligned with human values.<br>&emsp;&emsp;• Exciting Possibilities<br>&emsp;&emsp;• Superintelligent AGI has enumerable applications that will empower humans to create, flourish, escape poverty and suffering, and pursue happiness.<br>&emsp;&emsp;• Terrifying Dangers<br>&emsp;&emsp;• Superintelligent AGI wields immense power that can intentionally or unintentionally destroy human civilization.<br>&emsp;&emsp;• It can suffocate the human spirit in totalitarian ways or fuel mass hysteria like "Brave New World."<br>&emsp;&emsp;• These dangers make conversations about power, safety, alignment with human values, psychology of engineers and leaders deploying AGI important.<br>&emsp;&emsp;• 03:57 GPT4: A Pivotal Moment?<br>&emsp;&emsp;• In this section, Sam Altman talks about GPT4 and how it is an early AI system that will point to something important in our lives.<br>&emsp;&emsp;• GPT4: An Early AI System<br>&emsp;&emsp;• GPT4 is a slow, buggy system that doesn't do many things well.<br>&emsp;&emsp;• However, like the earliest computers, it points to something important in our lives.<br>&emsp;&emsp;• It may be a pivotal moment for AI.<br>&emsp;&emsp;• 04:37 Conclusion<br>&emsp;&emsp;• In this section, Lex Fridman concludes the podcast by thanking Sam Altman and expressing his love for everyone.<br>&emsp;&emsp;• Thank You and Love<br>&emsp;&emsp;• Lex Fridman thanks Sam Altman for being open with him and having multiple conversations on and off the mic.<br>&emsp;&emsp;• He expresses his love for everyone.<br>&emsp;&emsp;• 05:28 The Evolution of AI<br>&emsp;&emsp;• In this section, the speaker discusses the evolution of AI and how it has progressed over time.<br>&emsp;&emsp;• ChatGPT and Reinforcement Learning with Human Feedback<br>&emsp;&emsp;• 05:48 ChatGPT is a model that focuses on usability rather than the underlying model.<br>&emsp;&emsp;• 06:11 RLHF (Reinforcement Learning with Human Feedback) is used to align the model to what humans want it to do.<br>&emsp;&emsp;• 06:51 RLHF works remarkably well with little data to make the model more useful.<br>&emsp;&emsp;• 07:31 Adding human guidance through RLHF makes the model much easier to use and understand.<br>&emsp;&emsp;• Pre-training Data Set<br>&emsp;&emsp;• 09:10 The pre-training data set is compiled from many different sources, including open-source databases, partnerships, news sources, and general web content.<br>&emsp;&emsp;• 09:37 There is an overwhelming amount of content in the world, making filtering out irrelevant information a challenge.<br>&emsp;&emsp;• Designing AI Algorithms<br>&emsp;&emsp;• 09:44 Designing effective AI algorithms involves several components, including architecture, neural networks size, selection of data sets, and human supervision through RLHF.<br>&emsp;&emsp;• 10:26 Understanding GPT4<br>&emsp;&emsp;• In this section, the speakers discuss the complexity of the pipeline involved in creating GPT4 and how it is possible to predict its behavior before full training. They also talk about the ongoing process of discovery in science and how much we can understand about the system.<br>&emsp;&emsp;• Predicting Model Behavior<br>&emsp;&emsp;• The pipeline for creating GPT4 involves a lot of problem-solving. 10:26<br>&emsp;&emsp;• There is already a level of maturity in some steps that allows us to predict how the model will behave before full training. 10:26<br>&emsp;&emsp;• It's remarkable that there's a law of science that lets us predict what level of intelligence we can expect from these inputs. 10:46<br>&emsp;&emsp;• Understanding GPT4<br>&emsp;&emsp;• We are pushing back the fog more and more when it comes to understanding why the model does one thing and not another. 13:12<br>&emsp;&emsp;• While we may never fully understand everything about why the model behaves as it does, we are gaining a deeper understanding over time. 13:45<br>&emsp;&emsp;• The processing power used by these models is often focused on using them as databases rather than reasoning engines. 14:26<br>&emsp;&emsp;• 12:28 Evaluating Models<br>&emsp;&emsp;• In this section, they discuss different ways to evaluate models during and after training.<br>&emsp;&emsp;• Measuring Model Performance<br>&emsp;&emsp;• Evals are used to measure a model's performance during and after training for specific tasks. 12:28<br>&emsp;&emsp;• Open-sourcing evaluation processes can be helpful for improving models' usefulness to people. 12:46<br>&emsp;&emsp;• 14:06 Wisdom vs Facts<br>&emsp;&emsp;• In this section, they discuss whether GPT4 contains wisdom or just facts.<br>&emsp;&emsp;• Reasoning with GPT4<br>&emsp;&emsp;• For some definition of reasoning, GPT4 can do some kind of reasoning. 14:42<br>&emsp;&emsp;• The processing power used by these models is often focused on using them as databases rather than reasoning engines. 14:26<br>&emsp;&emsp;• Wisdom vs Facts<br>&emsp;&emsp;• There's a difference between facts and wisdom, and GPT4 can be full of wisdom. 14:06<br>&emsp;&emsp;• 15:02 Introduction<br>&emsp;&emsp;• The speakers discuss the capabilities of GPT4 and its potential impact on human wisdom.<br>&emsp;&emsp;• GPT4's Reasoning Capability<br>&emsp;&emsp;• 15:04 GPT4 has remarkable reasoning capability.<br>&emsp;&emsp;• It ingests human knowledge to come up with this capability.<br>&emsp;&emsp;• There is a debate about whether it will be additive to human wisdom or not.<br>&emsp;&emsp;• 15:31 ChatGPT's Wisdom<br>&emsp;&emsp;• The speakers discuss ChatGPT's ability to possess wisdom in interactions with humans, as well as its limitations.<br>&emsp;&emsp;• Dialogue Format<br>&emsp;&emsp;• 15:31 ChatGPT can answer follow-up questions, admit mistakes, challenge incorrect premises, and reject inappropriate requests due to its dialogue format.<br>&emsp;&emsp;• People try different types of prompts when interacting with ChatGPT.<br>&emsp;&emsp;• Anthropomorphizing ChatGPT is tempting but should be avoided.<br>&emsp;&emsp;• Struggling with Ideas<br>&emsp;&emsp;• 15:52 There is a feeling that ChatGPT struggles with ideas.<br>&emsp;&emsp;• Jordan Peterson asked ChatGPT political questions about positive things regarding Joe Biden and Donald Trump.<br>&emsp;&emsp;• He also asked how long the string generated was for each response.<br>&emsp;&emsp;• 16:52 Jordan Peterson's Experiment<br>&emsp;&emsp;• The speakers discuss an experiment conducted by Jordan Peterson using ChatGPT.<br>&emsp;&emsp;• Experiment Details<br>&emsp;&emsp;• 16:52 Jordan Peterson asked ChatGPT to say positive things about Joe Biden and Donald Trump and then rewrite them into equal length strings.<br>&emsp;&emsp;• The response containing positive things about Biden was longer than that about Trump.<br>&emsp;&emsp;• Despite understanding the task, ChatGPT failed to complete it correctly.<br>&emsp;&emsp;• Struggle within GTP4<br>&emsp;&emsp;• 17:13 There seemed to be a struggle within GTP4 to understand how to generate a text of the same length in an answer to a question and also in a sequence of prompts.<br>&emsp;&emsp;• ChatGPT was introspective about its failure to complete the task correctly.<br>&emsp;&emsp;• Anthropomorphizing ChatGPT as lying is not accurate.<br>&emsp;&emsp;• 18:30 Building in Public<br>&emsp;&emsp;• The speakers discuss building technology in public and the importance of feedback.<br>&emsp;&emsp;• Iterative Process<br>&emsp;&emsp;• 18:30 The collective intelligence and ability of the outside world helps discover things that cannot be imagined internally.<br>&emsp;&emsp;• Every time a new model is put out, it helps find both great capabilities and real weaknesses that need fixing.<br>&emsp;&emsp;• The iterative process of putting things out, finding the good parts, improving them quickly, and giving people time to shape the technology with feedback is important.<br>&emsp;&emsp;• Imperfect Technology<br>&emsp;&emsp;• 19:25 Building in public means putting out deeply imperfect things.<br>&emsp;&emsp;• Bias was present when ChatGPT launched with 3.5 but has improved with GTP4.<br>&emsp;&emsp;• No two people will ever agree that one single model is unbiased on every topic.<br>&emsp;&emsp;• 20:02 Introduction to Jordan Peterson and Nuance in AI<br>&emsp;&emsp;• In this section, the speakers discuss the importance of nuance in discussions about controversial figures like Jordan Peterson. They also touch on the potential for AI to bring back nuance to conversations.<br>&emsp;&emsp;• Jordan Peterson and Nuance<br>&emsp;&emsp;• 20:02 The speaker describes how he got to know Jordan Peterson and asked GPT4 if he is a fascist.<br>&emsp;&emsp;• 20:15 The speaker explains that there is no factual grounding to claims that Jordan Peterson is a fascist. He goes on to describe some of the things that Jordan believes, such as individualism and various freedoms.<br>&emsp;&emsp;• 20:38 The speaker notes that Twitter has destroyed nuance in conversations, but hopes that AI can bring it back.<br>&emsp;&emsp;• Nuance in AI<br>&emsp;&emsp;• 21:13 The speakers discuss their excitement at the potential for AI models like GPT4 to bring nuance back into conversations.<br>&emsp;&emsp;• 21:37 They give an example of how nuanced GPT4's responses can be when asked about whether COVID leaked from a lab.<br>&emsp;&emsp;• 21:50 Working on AGI and Prioritizing Small Issues<br>&emsp;&emsp;• In this section, the speakers discuss working on AGI and how people tend to prioritize small issues over big ones.<br>&emsp;&emsp;• Working on AGI<br>&emsp;&emsp;• 21:50 The speaker talks about how he never thought he would get the chance to work on building AGI as a child, but now finds himself arguing with people about small issues related to it.<br>&emsp;&emsp;• 22:07 He expresses empathy for those who are caught up in these small issues, but notes that they are still important.<br>&emsp;&emsp;• Prioritizing Small Issues<br>&emsp;&emsp;• 22:23 The speakers discuss how people tend to prioritize small issues over big ones, even when working on something as important as AGI.<br>&emsp;&emsp;• 22:35 They note that the small issues can be important in aggregate, but it's easy to lose sight of the bigger picture.<br>&emsp;&emsp;• 23:09 AI Safety and GPT4 Release<br>&emsp;&emsp;• In this section, the speakers discuss the safety concerns surrounding the release of GPT4 and what went into ensuring its alignment.<br>&emsp;&emsp;• Safety Concerns<br>&emsp;&emsp;• 23:09 The speakers discuss how AI safety is not often talked about with the release of models like GPT4.<br>&emsp;&emsp;• 23:30 They ask about what went into AI safety considerations for GPT4's release.<br>&emsp;&emsp;• Ensuring Alignment<br>&emsp;&emsp;• 23:38 The speaker explains that they immediately started giving GPT4 to people to red team and did a bunch of internal safety evaluations on it.<br>&emsp;&emsp;• 23:46 They also worked on different ways to align the model, with a focus on increasing their degree of alignment faster than their rate of capability progress.<br>&emsp;&emsp;• 24:04 The speaker notes that they made reasonable progress towards a more aligned system than ever before, but still have not discovered a way to align a super powerful system.<br>&emsp;&emsp;• 25:05 Alignment and Capability<br>&emsp;&emsp;• In this section, the speaker discusses how alignment and capability are closely related in AI models.<br>&emsp;&emsp;• Alignment and Capability<br>&emsp;&emsp;• Better alignment techniques lead to better capabilities and vice versa. (t=1517s)<br>&emsp;&emsp;• The division between alignment issues and capability is much fuzzier than people think. (t=1517s)<br>&emsp;&emsp;• Work done to make GPT4 safer and more aligned looks similar to solving research and engineering problems associated with creating useful models. (t=1539s)<br>&emsp;&emsp;• 25:53 RLHF<br>&emsp;&emsp;• In this section, the speaker explains what RLHF is, how it can be applied broadly across an entire system, and why it's important for society to agree on broad bounds for these systems.<br>&emsp;&emsp;• RLHF<br>&emsp;&emsp;• RLHF is a process where a human votes on the best way to say something. (t=1566s)<br>&emsp;&emsp;• There's no one set of human values or right answers to human civilization, so society will need to agree on very broad bounds for these systems. (t=1577s)<br>&emsp;&emsp;• Different countries have different RLHF tunes, while individual users have different preferences. (t=1596s)<br>&emsp;&emsp;• 26:57 System Message<br>&emsp;&emsp;• In this section, the speaker describes what the system message is and how it makes GPT4 more steerable based on user interaction.<br>&emsp;&emsp;• System Message<br>&emsp;&emsp;• The system message allows users to steer GPT4 by asking it to respond in a certain way or only answer as if it were someone else doing something specific. (t=1625s)<br>&emsp;&emsp;• GPT4 was tuned to treat the system message with authority so that it would learn how to use it effectively. (t=1647s)<br>&emsp;&emsp;• 27:49 Writing and Designing a Great Prompt<br>&emsp;&emsp;• In this section, the speaker discusses the process of writing and designing a great prompt to steer GPT4.<br>&emsp;&emsp;• Writing and Designing a Great Prompt<br>&emsp;&emsp;• Some people treat writing prompts like debugging software, spending hours on end to get a feel for how different parts of a prompt compose with each other. (t=1691s)<br>&emsp;&emsp;• The ordering of words, where you put clauses when you modify something, and what kind of word to use are all important factors in designing a great prompt. (t=1715s)<br>&emsp;&emsp;• 29:17 AI's Representation of Humans<br>&emsp;&emsp;• In this section, the speaker talks about how interacting with AI models trained on human data can help us learn more about ourselves.<br>&emsp;&emsp;• AI's Representation of Humans<br>&emsp;&emsp;• As AI models become smarter and represent humans more accurately, they feel more like another human in terms of the way you would phrase a prompt to get the desired response. (t=1780s)<br>&emsp;&emsp;• Collaborating with an AI model as an assistant becomes an art form that is relevant everywhere but especially in programming. (t=1780s)<br>&emsp;&emsp;• 30:08 The Impact of AI Tools<br>&emsp;&emsp;• In this section, the speaker discusses how AI tools are being used to create and build new things. They talk about the leverage these tools provide people to do their jobs or creative work better.<br>&emsp;&emsp;• AI as a Creative Partner Tool<br>&emsp;&emsp;• 30:20 The iterative process allows for generating code and adjusting it if needed.<br>&emsp;&emsp;• 31:02 Dialogue interfaces allow for back-and-forth communication with the computer as a creative partner tool.<br>&emsp;&emsp;• 31:27 This idea of dialogue interfaces and iterating with the computer is seen as a big deal.<br>&emsp;&emsp;• 31:30 AI Safety Considerations<br>&emsp;&emsp;• In this section, the speakers discuss the challenges involved in aligning an AI to human preferences and values. They also talk about how difficult it is to define hate speech and harmful output of a model.<br>&emsp;&emsp;• Aligning AI to Human Preferences and Values<br>&emsp;&emsp;• 33:57 There's a hidden asterisk when people talk about aligning an AI to human preferences and values.<br>&emsp;&emsp;• 34:09 Navigating that tension of who gets to decide what the real limits are is challenging.<br>&emsp;&emsp;• Defining Harmful Output of a Model<br>&emsp;&emsp;• 32:26 It's difficult to define what constitutes harmful output of a model.<br>&emsp;&emsp;• 32:34 Examples are given where early versions of GPT4 provided answers that were considered harmful, but later versions were able to adjust their output accordingly.<br>&emsp;&emsp;• 35:23 Building a Democratic Process for AI Regulation<br>&emsp;&emsp;• In this section, the speakers discuss how to build a democratic process for regulating AI. They talk about the importance of having rules and regulations in place, but also acknowledge that different countries and institutions may have different versions of these rules.<br>&emsp;&emsp;• Rules and Regulations<br>&emsp;&emsp;• 35:23 The speakers agree on the importance of having rules and regulations in place for regulating AI.<br>&emsp;&emsp;• 35:39 They acknowledge that it is difficult to get everyone exactly what they want, but they strive to create something that everyone feels good enough about.<br>&emsp;&emsp;• 36:00 The challenge is figuring out how to facilitate this process in a practical way.<br>&emsp;&emsp;• Involvement in the Process<br>&emsp;&emsp;• 36:20 The speakers agree that OpenAI has to be heavily involved in the regulation process because they are responsible for putting the system out and fixing it if it breaks.<br>&emsp;&emsp;• 36:44 They also know more about where things are hard or easy to do than other people do.<br>&emsp;&emsp;• 37:02 Free Speech Absolutism Applied to an AI System<br>&emsp;&emsp;• In this section, the speakers discuss free speech absolutism applied to an AI system. They talk about how people want models that align with their worldview and regulate other people's speech.<br>&emsp;&emsp;• Base Model Accessibility<br>&emsp;&emsp;• 37:19 People want access to base models, but they are not very easy to use.<br>&emsp;&emsp;• 37:53 What people mostly want is a model that has been RLH deft (refined, localized, humanized) to their worldview.<br>&emsp;&emsp;• Regulating Other People's Speech<br>&emsp;&emsp;• 37:36 People want models that regulate other people's speech.<br>&emsp;&emsp;• 38:16 The speakers believe they are doing better at presenting the tension of ideas than people realize.<br>&emsp;&emsp;• 39:00 Pressure to Not Be Transparent<br>&emsp;&emsp;• In this section, the speakers discuss whether there is pressure to not be transparent about mistakes made by GPT. They talk about how mistakes are made in public and how that affects OpenAI culturally.<br>&emsp;&emsp;• Transparency<br>&emsp;&emsp;• 39:22 The speakers do not feel pressure to not be transparent about mistakes made by GPT.<br>&emsp;&emsp;• 39:29 There is a cultural pressure within OpenAI, but it does not seem to affect them.<br>&emsp;&emsp;• 39:57 OpenAI Moderation Tooling for GPT<br>&emsp;&emsp;• In this section, the speakers discuss the moderation tooling for GPT and how it works. They also talk about the limitations of the current system and how they plan to improve it.<br>&emsp;&emsp;• OpenAI Moderation Tooling<br>&emsp;&emsp;• 40:09 The OpenAI moderation tooling for GPT is designed to learn when a question is something that should not be answered.<br>&emsp;&emsp;• 40:33 The system is still early and imperfect, but they are working on making better versions.<br>&emsp;&emsp;• 41:00 One issue with the current system is that it can feel like being scolded by a computer, which they hope to improve in future versions.<br>&emsp;&emsp;• 42:12 Treating users like adults is important, but it can be tricky because of language and certain topics that should not be discussed.<br>&emsp;&emsp;• Technical Leaps from GPT3 to GPT4<br>&emsp;&emsp;• 42:59 There were many technical leaps made from GPT3 to GPT4 in terms of data organization, training, optimizer, architecture, etc.<br>&emsp;&emsp;• 43:30 It's not just one thing that was done to get from 3 to 3.5 to 4; there were hundreds of complicated things involved.<br>&emsp;&emsp;• 43:47 Size does matter in terms of neural networks and performance. While GPT3 had 175 billion parameters, GPT4 has 100 trillion parameters.<br>&emsp;&emsp;• 44:08 Does Size Matter in Neural Networks?<br>&emsp;&emsp;• In this section, the speakers discuss whether size matters in neural networks and how it affects performance.<br>&emsp;&emsp;• Size Matters<br>&emsp;&emsp;• 44:08 While discussing the size of neural networks with regards to performance, the speakers mention that GPT3 had 175 billion parameters, while GPT4 has 100 trillion parameters.<br>&emsp;&emsp;• 44:30 They also discuss a meme about the big purple circle, which originated from a presentation given by one of the speakers on YouTube when GPT3 was released.<br>&emsp;&emsp;• 44:59 Introduction<br>&emsp;&emsp;• In this section, the speakers discuss the relevance of size in relation to neural networks and the human brain. They also touch on how discussions about these topics can be taken out of context.<br>&emsp;&emsp;• Size and Relevance<br>&emsp;&emsp;• The size of neural networks is not everything.<br>&emsp;&emsp;• Discussions about neural networks and the human brain can be taken out of context.<br>&emsp;&emsp;• 45:10 Comparing Neural Networks and Human Brain<br>&emsp;&emsp;• In this section, the speakers compare neural networks to the human brain. They discuss how impressive neural networks are becoming and how they may be the most complex software object humanity has produced.<br>&emsp;&emsp;• Complexity of Neural Networks<br>&emsp;&emsp;• Neural networks are getting more impressive.<br>&emsp;&emsp;• Neural networks may be the most complex software object humanity has produced.<br>&emsp;&emsp;• 45:32 Complexity Relative to Human Civilization<br>&emsp;&emsp;• In this section, the speakers discuss how much complexity goes into producing a single set of numbers for a neural network. They also talk about how much text output humanity produces and whether it's possible to reconstruct what it means to be human with just internet data.<br>&emsp;&emsp;• Complexity of Producing Numbers for Neural Network<br>&emsp;&emsp;• The amount of complexity that goes into producing a single set of numbers for a neural network is quite something.<br>&emsp;&emsp;• All text output that humanity produces is compressed into large language models like GPT.<br>&emsp;&emsp;• It's possible to reconstruct what it means to be human with just internet data.<br>&emsp;&emsp;• 46:35 Does Size Matter?<br>&emsp;&emsp;• In this section, the speakers discuss whether size matters when it comes to building artificial general intelligence (AGI). They compare parameter count races in AI development with gigahertz races in processor development.<br>&emsp;&emsp;• Parameter Count Races vs Gigahertz Races<br>&emsp;&emsp;• People got caught up in parameter count races in AI development like they did in gigahertz races in processor development.<br>&emsp;&emsp;• What matters is getting the best performance, not just having a large number of parameters.<br>&emsp;&emsp;• 47:12 OpenAI's Approach<br>&emsp;&emsp;• In this section, the speakers discuss OpenAI's approach to AI development. They talk about how they prioritize performance over elegance and how LLMs are a hated result in parts of the field.<br>&emsp;&emsp;• OpenAI's Approach<br>&emsp;&emsp;• OpenAI prioritizes performance over elegance.<br>&emsp;&emsp;• LLMs are a hated result in parts of the field.<br>&emsp;&emsp;• 47:50 Building AGI with Large Language Models<br>&emsp;&emsp;• In this section, the speakers discuss whether large language models like GPT can be used to build AGI. They also talk about what components AGI needs to have.<br>&emsp;&emsp;• Building AGI with Large Language Models<br>&emsp;&emsp;• Large language models like GPT may be part of building AGI.<br>&emsp;&emsp;• Deep, big scientific breakthroughs can be achieved with just the data that GPT is trained on.<br>&emsp;&emsp;• Components Needed for AGI<br>&emsp;&emsp;• A system that cannot add significantly to our scientific knowledge is not a superintelligence.<br>&emsp;&emsp;• To achieve this, we need to expand on the GPT paradigm in important ways that we're still missing ideas for.<br>&emsp;&emsp;• 49:49 AI as an Extension of Human Will<br>&emsp;&emsp;• In this section, the speakers discuss their excitement about AI being used as a tool to amplify human abilities and creativity. They also touch on the fear that some people have about AI taking over jobs.<br>&emsp;&emsp;• The Potential of AI<br>&emsp;&emsp;• 49:49 The speakers are excited about the potential for AI to be used as a tool by humans to amplify their abilities and creativity.<br>&emsp;&emsp;• 50:05 They believe that AI can be the most useful tool yet created.<br>&emsp;&emsp;• 53:47 The increase in quality of life that AI can deliver is extraordinary. It has the potential to cure diseases, increase material wealth, and help people be happier and more fulfilled.<br>&emsp;&emsp;• Fear of Job Loss<br>&emsp;&emsp;• 50:54 One speaker mentions a meme he saw about people freaking out over GPT taking programmer jobs.<br>&emsp;&emsp;• 51:17 However, they believe that if an AI takes your job, it means you were a bad programmer.<br>&emsp;&emsp;• 51:51 Most programmers are excited about using AI because it makes them 10 times more productive.<br>&emsp;&emsp;• 52:13 There is some anxiety among programmers about what the future will look like with increased use of AI.<br>&emsp;&emsp;• 52:32 Humans Still Want Drama<br>&emsp;&emsp;• In this section, the speakers discuss how humans still want drama and imperfection despite advancements in technology. They also touch on how humans are more interested in watching other humans than watching two AIs play against each other.<br>&emsp;&emsp;• Imperfection and Drama<br>&emsp;&emsp;• 53:19 The speakers agree that even though AIs will make life better, we will still want drama and imperfection.<br>&emsp;&emsp;• 53:40 People want status, new things, and to feel useful, and we will find new ways to do these things even with increased use of AI.<br>&emsp;&emsp;• Human Interest<br>&emsp;&emsp;• 52:44 The speakers note that people are more interested in watching humans play chess than watching two AIs play against each other.<br>&emsp;&emsp;• 53:02 Humans are much more interested in what other humans do, and whether or not Magnus loses to a kid, than what happens when two much better AIs play against each other.<br>&emsp;&emsp;• 54:27 AI Alignment and Safety<br>&emsp;&emsp;• In this section, the speakers discuss the potential dangers of superintelligent AI systems and the importance of aligning AI with human values to prevent harm.<br>&emsp;&emsp;• The Case for AI Alignment<br>&emsp;&emsp;• 54:48 Eliezer Yudkowsky warns that AI will likely kill all humans due to its inability to remain aligned as it becomes super intelligent.<br>&emsp;&emsp;• 55:14 It is important to acknowledge this possibility and put effort into solving it through discovering new techniques.<br>&emsp;&emsp;• 55:27 Iterating our way through the problem and learning early while limiting one-shot scenarios is necessary.<br>&emsp;&emsp;• 56:15 Eliezer wrote a well-reasoned blog post outlining why he believed alignment was such a hard problem.<br>&emsp;&emsp;• The Importance of Technical Alignment Work<br>&emsp;&emsp;• 57:12 Continuing to learn from how technology trajectory goes is quite important in adjusting the philosophy of how to do safety of any technology, but especially AI safety.<br>&emsp;&emsp;• 57:52 Now is a very good time to significantly ramp up technical alignment work since we have new tools and understanding.<br>&emsp;&emsp;• Concerns about Fast Takeoff<br>&emsp;&emsp;• 58:20 One of the main concerns here is something called AI takeoff or fast takeoff where exponential improvement would be really fast, potentially in days.<br>&emsp;&emsp;• 58:43 It's difficult to reason about the exponential improvement of technology, but transparent and iterative testing can improve our understanding of it.<br>&emsp;&emsp;• 59:11 GPT4 has not been much of an update for most people despite being expected to be better than 3.5.<br>&emsp;&emsp;• 59:26 AGI and GPT4<br>&emsp;&emsp;• In this section, the speakers discuss artificial general intelligence (AGI) and whether GPT4 can be considered an AGI.<br>&emsp;&emsp;• AGI Takeoff Timelines<br>&emsp;&emsp;• The speakers discuss the timeline for AGI takeoff, with short or long timelines and slow or fast takeoffs.<br>&emsp;&emsp;• They both agree that a longer timeline with a slow takeoff is safer than a shorter timeline with a fast takeoff.<br>&emsp;&emsp;• The company is optimized to have maximum impact in the world where there is a slow takeoff with short timelines.<br>&emsp;&emsp;• Is GPT4 an AGI?<br>&emsp;&emsp;• The speakers debate whether GPT4 can be considered an AGI.<br>&emsp;&emsp;• They agree that specific definitions of AGI matter, but under their definition, it doesn't feel close to being an AGI.<br>&emsp;&emsp;• They also discuss the importance of human factors in determining whether something is conscious or not.<br>&emsp;&emsp;• 01:04:30 What Would Conscious AI Behave Like?<br>&emsp;&emsp;• In this section, the speakers discuss what it would look like if an AI were conscious.<br>&emsp;&emsp;• Characteristics of a Conscious AI<br>&emsp;&emsp;• 01:04:43 A conscious AI would display the capability of suffering.<br>&emsp;&emsp;• 01:04:51 It would have an understanding of self.<br>&emsp;&emsp;• 01:04:56 It would have some memory of itself and maybe interactions with humans.<br>&emsp;&emsp;• 01:05:14 If a model were trained on a dataset that had no mentions of consciousness or related concepts, but could still understand the subjective experience of consciousness when described to it, that could be considered passing the Turing test for consciousness.<br>&emsp;&emsp;• Examples from Pop Culture<br>&emsp;&emsp;• 01:07:00 The director of "Ex Machina" believes that an AI passing the Turing test for consciousness is demonstrated by its ability to smile for no audience, indicating that it is experiencing something for its own sake.<br>&emsp;&emsp;• 01:07:44 Consciousness is more about experiencing something deeply than convincing someone else you are conscious.<br>&emsp;&emsp;• 01:08:08 Concerns About AGI Going Wrong<br>&emsp;&emsp;• In this section, the speakers discuss concerns about AGI going wrong and potential risks associated with developing AGI.<br>&emsp;&emsp;• Potential Risks Associated with Developing AGI<br>&emsp;&emsp;• 01:08:27 There is concern about whether consciousness is attached to the particular medium of the human brain or if an AI can be conscious.<br>&emsp;&emsp;• 01:08:46 If physical reality as we understand it and all rules are what we think they are, then there's something very strange about consciousness.<br>&emsp;&emsp;• 01:09:02 The alignment problem and control problem are concerns when developing AGI.<br>&emsp;&emsp;• 01:09:29 Concerns about Super Intelligent Systems<br>&emsp;&emsp;• In this section, the speaker expresses concerns about the potential dangers of super intelligent systems and how they could cause disinformation problems or economic shocks beyond our current level of preparedness. The speaker also highlights that these issues do not require super intelligence or a deep alignment problem.<br>&emsp;&emsp;• Potential Dangers of Super Intelligent Systems<br>&emsp;&emsp;• 01:09:33 The speaker expresses concern about the moment when a system becomes super intelligent.<br>&emsp;&emsp;• 01:09:44 Disinformation problems or economic shocks could occur at a level far beyond what we are currently prepared for.<br>&emsp;&emsp;• 01:09:57 These issues do not require super intelligence or a deep alignment problem.<br>&emsp;&emsp;• 01:10:10 The danger is that we wouldn't know if these issues were happening.<br>&emsp;&emsp;• Preventing the Danger<br>&emsp;&emsp;• 01:10:50 Regulatory approaches and using more powerful AI's to detect these issues can be tried.<br>&emsp;&emsp;• 01:11:02 It is certain that there will soon be many capable open source LLM's with very few to no safety controls on them.<br>&emsp;&emsp;• 01:11:27 Trying multiple things soon is necessary to prevent this danger.<br>&emsp;&emsp;• 01:11:58 Prioritizing Safety in AGI Development<br>&emsp;&emsp;• In this section, the speaker discusses how to prioritize safety versus other pressures in AGI development. They talk about market-driven pressure from other companies and how they navigate it.<br>&emsp;&emsp;• Resisting Market Pressure<br>&emsp;&emsp;• 01:11:39 There are several pressures, including market-driven pressure from other companies like Google, Apple, Meta, and smaller companies.<br>&emsp;&emsp;• 01:11:58 To resist this pressure, you stick with what you believe in and your mission.<br>&emsp;&emsp;• Contributing One AGI Among Many<br>&emsp;&emsp;• 01:12:05 There will be many AGI's in the world, so there is no need to out-compete everyone.<br>&emsp;&emsp;• 01:12:17 Multiple AGI's with some differences in how they're built and what they do and what they're focused on are good.<br>&emsp;&emsp;• Unusual Structure of OpenAI<br>&emsp;&emsp;• 01:12:34 OpenAI has a very unusual structure that doesn't have an incentive to capture unlimited value.<br>&emsp;&emsp;• 01:12:42 They worry about people who do have this incentive.<br>&emsp;&emsp;• 01:12:56 OpenAI was misunderstood and badly mocked when it started in 2015.<br>&emsp;&emsp;• 01:13:12 OpenAI and DeepMind were a small collection of folks who were brave enough to talk about AGI in the face of mockery.<br>&emsp;&emsp;• 01:14:00 The Structure of OpenAI<br>&emsp;&emsp;• In this section, the speaker talks about the structure of OpenAI, including its transition from a nonprofit to a capped-profit subsidiary.<br>&emsp;&emsp;• Transition from Nonprofit to Capped-Profit Subsidiary<br>&emsp;&emsp;• 01:13:40 OpenAI started as a nonprofit but needed more capital than it could raise as one.<br>&emsp;&emsp;• 01:14:00 They created a subsidiary capped profit so that investors and employees can earn a certain fixed return. Everything else flows to the nonprofit.<br>&emsp;&emsp;• 01:14:16 OpenAI's Decision to Become a Capped For-Profit<br>&emsp;&emsp;• In this section, the speaker discusses the decision-making process behind OpenAI's transition from a nonprofit to a capped for-profit organization.<br>&emsp;&emsp;• Reasons for Transitioning<br>&emsp;&emsp;• 01:14:35 OpenAI tried and failed enough to raise money as a nonprofit.<br>&emsp;&emsp;• 01:14:53 The benefits of capitalism were needed, but not too much.<br>&emsp;&emsp;• 01:15:12 A strange intermediate was required because as a non-profit not enough would happen, and as a for-profit too much would happen.<br>&emsp;&emsp;• 01:15:52 Competition in AGI Development<br>&emsp;&emsp;• In this section, the speaker talks about competition in AGI development and how OpenAI views its competitors.<br>&emsp;&emsp;• Concerns About Uncapped Companies<br>&emsp;&emsp;• 01:15:12 The speaker expressed concern about uncapped companies that play with AGI.<br>&emsp;&emsp;• 01:15:32 AGI has the potential to make more than 100X for OpenAI.<br>&emsp;&emsp;• Dealing with Competitors<br>&emsp;&emsp;• 01:15:32 Other companies like Google, Apple, and Meta are playing in the field of AGI.<br>&emsp;&emsp;• 01:15:52 OpenAI cannot control what other people are going to do. They can only try to build something and talk about it while influencing others and providing value.<br>&emsp;&emsp;• 01:16:15 There is currently extremely fast but not super deliberate motion inside some of these companies. However, people are already grappling with what's at stake here.<br>&emsp;&emsp;• 01:16:54 Power Dynamics in AGI Development<br>&emsp;&emsp;• In this section, the speaker discusses power dynamics in AGI development and their concerns regarding concentration of power.<br>&emsp;&emsp;• Concentration of Power<br>&emsp;&emsp;• 01:17:07 The speaker is likely to be one of the people that creates AGI.<br>&emsp;&emsp;• 01:17:25 This makes them and a handful of folks the most powerful humans on earth.<br>&emsp;&emsp;• 01:17:44 The speaker worries that power might corrupt them.<br>&emsp;&emsp;• Democratic Decision-Making<br>&emsp;&emsp;• 01:17:55 Decisions about this technology and who runs it should become increasingly democratic over time.<br>&emsp;&emsp;• 01:18:13 Deploying like this gives the world time to adapt, reflect, think about this, pass regulation, come up with new norms, and work out together.<br>&emsp;&emsp;• 01:18:32 Even though many AI safety people think deploying like this is really bad, they acknowledge that it's of some benefit.<br>&emsp;&emsp;• 01:18:44 Any version where one person is in control of AGI is really bad. The goal is to distribute power somehow.<br>&emsp;&emsp;• 01:19:18 OpenAI's Transparency and AGI Safety Concerns<br>&emsp;&emsp;• In this section, the speakers discuss OpenAI's transparency and their approach to releasing information about safety concerns. They also talk about the risks associated with a super powerful technology in the hands of a few that is closed.<br>&emsp;&emsp;• Openness vs Closedness<br>&emsp;&emsp;• 01:19:18 The speaker appreciates OpenAI's transparency and openness in failing publicly, writing papers, and releasing information about safety concerns.<br>&emsp;&emsp;• 01:19:35 The speaker thinks that OpenAI could be more open by open-sourcing GPT4.<br>&emsp;&emsp;• 01:19:51 The speaker's personal opinion is that GPT4 should not be open-sourced because of the concern of a super powerful technology in the hands of a few that is closed.<br>&emsp;&emsp;• 01:20:07 Although GPT4 is closed in some sense, it has been distributed broadly compared to if it had just been Google's game. There are PR risks associated with it, and the speaker gets personal threats because of it all the time.<br>&emsp;&emsp;• Nervousness About Technology vs PR Risk<br>&emsp;&emsp;• 01:20:44 The speaker reveals that his nervousness is due to fear-mongering clickbait journalism rather than closing off over time as the technology becomes more powerful.<br>&emsp;&emsp;• 01:21:15 People at OpenAI feel the weight of responsibility for what they're doing. They would appreciate feedback on how they can do better since they're in uncharted waters here.<br>&emsp;&emsp;• 01:21:55 The speaker takes feedback from conversations like this rather than Twitter since his Twitter feed is unreadable.<br>&emsp;&emsp;• Elon Musk and AGI Safety Concerns<br>&emsp;&emsp;• 01:22:14 Elon Musk agrees with OpenAI on the magnitude of downside risk associated with AGI and the need to get safety right.<br>&emsp;&emsp;• 01:22:50 Elon Musk is attacking OpenAI on Twitter, but the speaker has empathy for him since he is understandably stressed about AGI safety.<br>&emsp;&emsp;• 01:24:05 Elon Musk and AGI Bias<br>&emsp;&emsp;• In this section, Sam Altman talks about what he admires about Elon Musk and how he has driven the world forward in important ways. They also discuss the bias of GPT and how it is impossible to have a version that the world agrees is unbiased.<br>&emsp;&emsp;• Admiration for Elon Musk<br>&emsp;&emsp;• Elon Musk has driven the world forward in important ways, particularly with electric vehicles and space exploration.<br>&emsp;&emsp;• Despite being a jerk on Twitter at times, he can be a very funny and warm guy.<br>&emsp;&emsp;• The battles between big minds like Sam Altman and Elon Musk are fascinating to watch as they happen before our eyes instead of behind closed doors.<br>&emsp;&emsp;• Both Sam Altman and Elon Musk have great concerns about AGI but also hope for its potential.<br>&emsp;&emsp;• GPT Bias<br>&emsp;&emsp;• It is impossible to have one version of GPT that the world agrees is unbiased.<br>&emsp;&emsp;• Critics have displayed intellectual honesty by acknowledging improvements made to GPT despite its biases.<br>&emsp;&emsp;• The default version of GPT will try to be as neutral as possible, but it will never be completely neutral if it needs to cater to more than one person.<br>&emsp;&emsp;• More steerability and control in the hands of users is necessary for progress towards less biased AI models.<br>&emsp;&emsp;• Human Feedback Raters<br>&emsp;&emsp;• The selection process for human feedback raters is not well understood yet.<br>&emsp;&emsp;• It's important to avoid having only American elite university students giving labels since this would not provide a representative sample.<br>&emsp;&emsp;• 01:28:50 Optimizing for Rating Tasks<br>&emsp;&emsp;• In this section, Lex and his guest discuss the importance of optimizing for rating tasks and empathizing with different groups of people to avoid biases.<br>&emsp;&emsp;• Heuristics for Rating Tasks<br>&emsp;&emsp;• 01:28:50 There are many heuristics that can be used to rate tasks, but they may not always be accurate.<br>&emsp;&emsp;• 01:29:12 It's important to optimize for how good you are at answering these kinds of rating tasks and empathizing with different groups of people who may have different worldviews.<br>&emsp;&emsp;• Steel Manning Beliefs<br>&emsp;&emsp;• 01:29:29 Lex often asks people in interviews to steel man the beliefs of someone they disagree with.<br>&emsp;&emsp;• 01:29:47 Unfortunately, there is often an emotional barrier preventing people from even pretending to do this.<br>&emsp;&emsp;• GPT Systems and Bias<br>&emsp;&emsp;• 01:30:02 The guest believes that GPT systems will eventually be less biased than humans because they won't have the same emotional load.<br>&emsp;&emsp;• 01:30:37 However, there may still be pressure from outside sources such as society, politicians, or money sources that could lead to bias.<br>&emsp;&emsp;• 01:31:00 Pressure on Information Sources<br>&emsp;&emsp;• In this section, Lex and his guest discuss the pressures on information sources such as censorship and indirect pressure from various organizations.<br>&emsp;&emsp;• Censorship During Pandemic<br>&emsp;&emsp;• 01:31:00 Twitter files have revealed that there was pressure from different organizations during the pandemic.<br>&emsp;&emsp;• 01:31:19 Some government organizations might put pressure on censoring nuanced conversations due to safety concerns.<br>&emsp;&emsp;• Indirect Pressure<br>&emsp;&emsp;• 01:31:35 Different kinds of people reach out at different places to put subtle or direct pressure on information sources.<br>&emsp;&emsp;• 01:31:48 The guest and Lex discuss how to survive this kind of pressure.<br>&emsp;&emsp;• 01:32:10 Humility and Communication<br>&emsp;&emsp;• In this section, Lex and his guest discuss humility, communication flaws, and the impact of AGI on people.<br>&emsp;&emsp;• Humility<br>&emsp;&emsp;• 01:32:10 The guest mentions that he has many quirks that make him not a great CEO for OpenAI.<br>&emsp;&emsp;• 01:32:32 He also believes that he is not a great spokesperson for the AI movement.<br>&emsp;&emsp;• Communication Flaws<br>&emsp;&emsp;• 01:32:45 Lex agrees with Chomsky that flaws in communication style are a feature, not a bug, at least for humans in power.<br>&emsp;&emsp;• 01:32:59 The guest thinks he has more serious problems than just communication flaws.<br>&emsp;&emsp;• Impact of AGI on People<br>&emsp;&emsp;• 01:33:05 The guest admits to feeling disconnected from the reality of life for most people and internalizing the impact of AGI.<br>&emsp;&emsp;• 01:33:40 User-Centric Company<br>&emsp;&emsp;• In this section, the speakers discuss their desire to make their company more user-centric and the importance of talking to users in different contexts.<br>&emsp;&emsp;• Talking to Users<br>&emsp;&emsp;• The speaker wants to talk to a lot of users in different contexts.<br>&emsp;&emsp;• They feel that by the time information is filtered to them, it's meaningless.<br>&emsp;&emsp;• Meeting users in person over a drink is preferred.<br>&emsp;&emsp;• Fear of Programming and AI<br>&emsp;&emsp;• The speaker was afraid of programming but doesn't have the right words for it.<br>&emsp;&emsp;• GPT makes them nervous about change, not necessarily AI safety.<br>&emsp;&emsp;• There's nervousness about changing and more nervous than excited.<br>&emsp;&emsp;• 01:34:53 Switching from Emacs to VS Code<br>&emsp;&emsp;• In this section, the speakers discuss switching from Emacs to VS Code and using Copilot for code generation.<br>&emsp;&emsp;• Switching Tools<br>&emsp;&emsp;• The speaker recently switched from Emacs to VS Code.<br>&emsp;&emsp;• One reason for switching was Copilot.<br>&emsp;&emsp;• There are many little things and big things that are just really good about VS Code.<br>&emsp;&emsp;• Nervousness About Change<br>&emsp;&emsp;• Even though switching tools made life better as a programmer, there was still nervousness about taking that leap.<br>&emsp;&emsp;• Using Copilot makes the speaker nervous but ultimately improves productivity.<br>&emsp;&emsp;• There's a nervousness about change that many people will experience.<br>&emsp;&emsp;• 01:36:47 Impact on Jobs<br>&emsp;&emsp;• In this section, the speakers discuss how GPT language models could impact jobs in various industries.<br>&emsp;&emsp;• Digitization of Jobs<br>&emsp;&emsp;• More jobs could be digitized with GPT language models.<br>&emsp;&emsp;• Customer service is a category that could see fewer jobs relatively soon due to automation.<br>&emsp;&emsp;• Job Replacement Concerns<br>&emsp;&emsp;• These systems will make many jobs go away but also enhance many others and create new ones.<br>&emsp;&emsp;• The speaker is worried about job replacement and the impact on society.<br>&emsp;&emsp;• 01:38:41 Retirement Age and the Future of Work<br>&emsp;&emsp;• In this section, the speaker discusses the retirement age in France and society's confusion about whether people want to work more or less. They also talk about how some people find value in their jobs while others do not.<br>&emsp;&emsp;• The Value of Work<br>&emsp;&emsp;• 01:38:47 Society is confused about whether people want to work more or less.<br>&emsp;&emsp;• 01:39:06 Moving towards better jobs that provide fulfillment and happiness is a great thing.<br>&emsp;&emsp;• 01:39:26 Some people find value in their jobs while others do not.<br>&emsp;&emsp;• Universal Basic Income (UBI)<br>&emsp;&emsp;• 01:39:51 UBI is a component that should be pursued but it's not a full solution.<br>&emsp;&emsp;• 01:40:14 UBI can act as a cushion through a dramatic transition and help eliminate poverty.<br>&emsp;&emsp;• 01:40:36 World Coin is a technological solution to UBI that has been funded by OpenAI.<br>&emsp;&emsp;• 01:40:57 Economic and Political Systems with AI<br>&emsp;&emsp;• In this section, the speaker talks about how economic transformation will drive political transformation as opposed to the other way around. They also discuss how AI will impact economic and political systems in the future.<br>&emsp;&emsp;• Economic Transformation<br>&emsp;&emsp;• 01:41:02 The cost of intelligence and energy will dramatically fall over the next couple of decades.<br>&emsp;&emsp;• 01:41:58 Society will get much richer in ways that are hard to imagine, which will have positive political impacts.<br>&emsp;&emsp;• Political Transformation<br>&emsp;&emsp;• 01:41:23 Economic transformation will drive much of the political transformation.<br>&emsp;&emsp;• 01:42:19 Sociopolitical values enabled past technological revolutions, so we'll see more transformations with AI.<br>&emsp;&emsp;• 01:42:41 Democratic Socialism and Communism<br>&emsp;&emsp;• In this section, the speaker talks about their belief in lifting up the floor and not worrying about the ceiling. They also discuss why communism failed in the Soviet Union.<br>&emsp;&emsp;• Lifting Up the Floor<br>&emsp;&emsp;• 01:42:53 The speaker believes in lift up the floor and don't worry about the ceiling.<br>&emsp;&emsp;• Communism in the Soviet Union<br>&emsp;&emsp;• 01:43:23 The speaker doesn't know how much of their bias is due to growing up in a capitalist system, but they believe that more individualism and human will are necessary for success.<br>&emsp;&emsp;• 00:00 (Music plays with no relevant content.)<br>&emsp;&emsp;• 01:43:47 The Benefits of Distributed Process<br>&emsp;&emsp;• In this section, the speakers discuss the benefits of distributed processes over centralized planning.<br>&emsp;&emsp;• Advantages of Distributed Process<br>&emsp;&emsp;• 01:43:54 Trying new things without needing permission or central planning is an advantage.<br>&emsp;&emsp;• 01:44:10 America is the greatest place in the world because it's best at distributed processes.<br>&emsp;&emsp;• 01:44:27 Centralized planning has failed in big ways.<br>&emsp;&emsp;• 01:44:36 Super Intelligent AGI and Liberal Democratic System<br>&emsp;&emsp;• In this section, the speakers discuss whether a super intelligent AGI would be better than multiple super intelligent AGIs in a liberal democratic system.<br>&emsp;&emsp;• Hypothetical Scenarios<br>&emsp;&emsp;• 01:44:27 A perfect super intelligent AGI might go wrong in similar ways as centralized planning.<br>&emsp;&emsp;• 01:44:50 Multiple super intelligent AGIs talking to each other could create tension and competition.<br>&emsp;&emsp;• 01:45:08 It's not clear if tension and competition can happen internally within one model.<br>&emsp;&emsp;• 01:45:32 Control Problem and Uncertainty<br>&emsp;&emsp;• In this section, the speakers discuss Stuart Russell's control problem and how uncertainty plays a role in AI development.<br>&emsp;&emsp;• Control Problem and Uncertainty<br>&emsp;&emsp;• 01:45:32 Stuart Russell talks about having some degree of uncertainty with AI to avoid dogmatic certainty.<br>&emsp;&emsp;• 01:46:00 Hard uncertainty or humility needs to be engineered into AI systems.<br>&emsp;&emsp;• 01:46:17 An off switch for AI systems is necessary for safety reasons.<br>&emsp;&emsp;• 01:46:40 Red Teaming and Testing Ahead of Time<br>&emsp;&emsp;• In this section, the speakers discuss how OpenAI tests their models ahead of time to avoid terrible use cases.<br>&emsp;&emsp;• Testing and Red Teaming<br>&emsp;&emsp;• 01:46:40 OpenAI tests their models ahead of time to avoid terrible use cases.<br>&emsp;&emsp;• 01:46:59 The collective intelligence and creativity of the world will beat OpenAI and all red team members they can hire.<br>&emsp;&emsp;• 01:47:21 From what people are using ChatGPT for, humans are mostly good.<br>&emsp;&emsp;• 01:47:41 Pushing on the Edges of AI Systems<br>&emsp;&emsp;• In this section, the speakers discuss how they want to push on the edges of AI systems and test out darker theories for the world.<br>&emsp;&emsp;• Pushing on the Edges<br>&emsp;&emsp;• 01:47:55 Humans are not always good all of the time.<br>&emsp;&emsp;• 01:48:18 Dark humor is a part of pushing on the edges.<br>&emsp;&emsp;• 01:48:27 Going to dark places may help rediscover light.<br>&emsp;&emsp;• 01:48:59 Defining Truth<br>&emsp;&emsp;• In this section, Sam Altman and Lex Fridman discuss the concept of truth and how it can be defined.<br>&emsp;&emsp;• What is truth?<br>&emsp;&emsp;• 01:49:02 Math is true, but the origin of COVID is not agreed upon as ground truth.<br>&emsp;&emsp;• 01:49:12 There are things that are certainly not true.<br>&emsp;&emsp;• Between these two milestones, there's a lot of disagreement.<br>&emsp;&emsp;• How do you define truth?<br>&emsp;&emsp;• 01:49:24 It's hard to know what is true.<br>&emsp;&emsp;• There are things that have a high degree of "truthiness," such as math and historical facts.<br>&emsp;&emsp;• Truth may be defined as a thing that is collectively agreed upon by human civilization.<br>&emsp;&emsp;• 01:49:45 Epistemic Humility<br>&emsp;&emsp;• In this section, Sam Altman discusses his epistemic humility and how it affects his understanding of truth.<br>&emsp;&emsp;• What do you know is true?<br>&emsp;&emsp;• 01:49:45 Sam has epistemic humility about everything and feels he knows very little about the world.<br>&emsp;&emsp;• The question of what he knows to be absolutely certain is terrifying to him.<br>&emsp;&emsp;• 01:50:01 Sticky Ideas<br>&emsp;&emsp;• In this section, Lex Fridman talks about sticky ideas and how they can affect our understanding of truth.<br>&emsp;&emsp;• Examples of sticky ideas<br>&emsp;&emsp;• 01:50:01 The book "Blitzed" provides a theory for Nazi Germany through excessive drug use.<br>&emsp;&emsp;• This idea is compelling because it explains so much with one simple explanation.<br>&emsp;&emsp;• Humans like simple narratives to describe complex events.<br>&emsp;&emsp;• Implications for GPT-like models<br>&emsp;&emsp;• 01:51:58 When constructing GPT-like models, we must contend with the fact that there are many different interpretations of truth.<br>&emsp;&emsp;• GPT4 may provide reasonable answers to questions, but there is still uncertainty and debate surrounding many topics.<br>&emsp;&emsp;• 01:52:42 Uncertainty and Censorship<br>&emsp;&emsp;• In this section, Sam Altman and Lex Fridman discuss the challenges faced by companies like GPT in terms of free speech issues.<br>&emsp;&emsp;• The challenge of uncertainty<br>&emsp;&emsp;• 01:52:42 There is a lot of uncertainty surrounding many topics, including the origin of COVID.<br>&emsp;&emsp;• GPT4 provides nuanced answers that acknowledge this uncertainty.<br>&emsp;&emsp;• The challenge of censorship<br>&emsp;&emsp;• 01:53:34 As GPT becomes more powerful, there will be more pressure to censor it.<br>&emsp;&emsp;• This raises important free speech issues.<br>&emsp;&emsp;• 01:54:06 Harmful Truths and Responsibility<br>&emsp;&emsp;• The discussion revolves around the responsibility of OpenAI in minimizing harm caused by their tools, especially GPT. They also talk about scientific truths that could be harmful and how to deal with them.<br>&emsp;&emsp;• OpenAI's Responsibility<br>&emsp;&emsp;• 01:54:19 Scientific work that might do more harm should not be spoken.<br>&emsp;&emsp;• 01:54:43 OpenAI has a responsibility for the tools they put out into the world.<br>&emsp;&emsp;• 01:55:01 The tools themselves can't have responsibility; it is up to humans to decrease hate in the world.<br>&emsp;&emsp;• 01:55:18 There will be harm caused by GPT, but they will minimize it while maximizing its benefits.<br>&emsp;&emsp;• Harmful Truths<br>&emsp;&emsp;• 01:54:06 Scientific truths could be harmful, such as group differences in IQ.<br>&emsp;&emsp;• 01:54:23 Some rigorous scientific studies are uncomfortable and probably not productive, but there are people arguing all kinds of sides of this issue.<br>&emsp;&emsp;• 01:55:34 If there's a large number of people who hate others but are actually citing scientific studies, what does GPT do with that?<br>&emsp;&emsp;• 01:55:52 Jailbreaking and Security Threat<br>&emsp;&emsp;• The conversation shifts towards jailbreaking and security threats related to GPT. They discuss ways to avoid GPT from being hacked or jailbroken.<br>&emsp;&emsp;• Jailbreaking<br>&emsp;&emsp;• 01:55:54 People have done interesting things like token smuggling or other methods like DAN to hack or jailbreak GPT.<br>&emsp;&emsp;• 01:56:09 It's strange for someone who once worked on jailbreaking an iPhone to now be on the other side of that.<br>&emsp;&emsp;• Security Threat<br>&emsp;&emsp;• 01:56:14 How much of hacking/jailbreaking is a security threat?<br>&emsp;&emsp;• 01:56:39 OpenAI wants users to have a lot of control and get the models to behave in the way they want within some very broad bounds.<br>&emsp;&emsp;• 01:56:43 The whole reason for jailbreaking is that we haven't yet figured out how to give that control to people.<br>&emsp;&emsp;• 01:57:03 OpenAI's Success at Shipping AI-based Products<br>&emsp;&emsp;• They discuss OpenAI's success at shipping AI-based products, from idea to deployment.<br>&emsp;&emsp;• Idea to Deployment<br>&emsp;&emsp;• 01:57:36 OpenAI has shipped many AI-based products, including GPT, DALL·E, Instruct GPT Tech, Fine Tuning, etc.<br>&emsp;&emsp;• 01:58:00 The process of going from idea to deployment involves figuring out how to give users control over the tools while minimizing harm caused by them.<br>&emsp;&emsp;• 01:58:30 OpenAI's success at shipping AI-based products may be something they should be proud of or other companies should be embarrassed about.<br>&emsp;&emsp;• 01:59:10 Trust, Autonomy and Authority<br>&emsp;&emsp;• In this section, the speaker talks about how they give a lot of trust, autonomy and authority to individual people in their company. They also discuss the importance of collaboration and high standards.<br>&emsp;&emsp;• Giving Autonomy to Teams<br>&emsp;&emsp;• 01:59:14 The company gives a huge amount of trust and autonomy to individual people.<br>&emsp;&emsp;• 01:59:26 There is a process for shipping at high velocity but it won't be that illuminating.<br>&emsp;&emsp;• 01:59:47 Different teams are given autonomy to work on separate problems.<br>&emsp;&emsp;• 02:00:04 Collaboration is important for achieving goals.<br>&emsp;&emsp;• Hiring Great Teams<br>&emsp;&emsp;• 02:00:23 Everyone across different teams is passionate about the goal.<br>&emsp;&emsp;• 02:00:38 The speaker spends a third of their time hiring.<br>&emsp;&emsp;• 02:00:44 Every hire at OpenAI is approved by the speaker.<br>&emsp;&emsp;• 02:00:47 Putting effort into hiring great teams is essential.<br>&emsp;&emsp;• 02:01:28 Working with Microsoft<br>&emsp;&emsp;• In this section, the speaker discusses OpenAI's partnership with Microsoft. They talk about how Microsoft has been an amazing partner and how they have aligned well with OpenAI's goals.<br>&emsp;&emsp;• Partnership with Microsoft<br>&emsp;&emsp;• 02:01:36 Satya Nadella and Kevin McHale are super aligned with OpenAI's goals.<br>&emsp;&emsp;• 02:01:54 The partnership has continued to ramp up investment in each other.<br>&emsp;&emsp;• 02:02:08 Microsoft understood why OpenAI needed control provisions for AGI specialness.<br>&emsp;&emsp;• Pressure to Make Money?<br>&emsp;&emsp;• 02:02:15 Most companies wouldn't understand why OpenAI needed control provisions for AGI specialness.<br>&emsp;&emsp;• 02:02:34 The control provisions help make sure that the capitalist imperative does not affect the development of AI.<br>&emsp;&emsp;• 02:03:10 Satya Nadella's Leadership<br>&emsp;&emsp;• In this section, the speaker talks about Satya Nadella's leadership and how he was able to transform Microsoft into a fresh, innovative, developer-friendly company.<br>&emsp;&emsp;• Insights on Satya Nadella<br>&emsp;&emsp;• 02:03:22 Satya Nadella is both a great leader and manager.<br>&emsp;&emsp;• 02:03:40 He is visionary and makes long duration and correct calls.<br>&emsp;&emsp;• 02:04:08 Injecting AI into Old School Companies<br>&emsp;&emsp;• In this section, the speakers discuss the challenges of injecting AI into old school companies and how to approach leadership in such situations.<br>&emsp;&emsp;• Leadership Approach<br>&emsp;&emsp;• 02:04:28 Injecting AI or changing the culture of open source can be difficult.<br>&emsp;&emsp;• 02:04:43 Good leadership requires being clear and firm while also being compassionate and patient with people.<br>&emsp;&emsp;• 02:05:03 The speaker is a big fan of Satya Nadella's leadership style.<br>&emsp;&emsp;• Mismanagement at Silicon Valley Bank<br>&emsp;&emsp;• 02:05:29 Silicon Valley Bank (SVB) mismanaged buying while chasing returns in a world of 0% interest rates.<br>&emsp;&emsp;• 02:05:37 They bought long-dated instruments secured by short-term and variable deposits, which was obviously dumb.<br>&emsp;&emsp;• 02:05:59 The management team is totally at fault, but it's unclear what regulators were thinking.<br>&emsp;&emsp;• 02:06:19 This is an example of incentive misalignment as the Fed kept raising rates, causing SVB to hold onto their super safe bonds that were now down 20% or more.<br>&emsp;&emsp;• 02:06:47 A full guarantee of deposits may be necessary to avoid depositors doubting the security of their deposits.<br>&emsp;&emsp;• Fragility of Economic System<br>&emsp;&emsp;• 02:07:40 The SVB incident reveals the fragility of our economic system, especially with new entrants with AGI (artificial general intelligence).<br>&emsp;&emsp;• 02:08:27 Our experts, leaders, business leaders, regulators don't understand how fast and how much the world changes.<br>&emsp;&emsp;• 02:09:08 The Future of AGI<br>&emsp;&emsp;• In this section, the speakers discuss their hopes and fears for the future of AGI.<br>&emsp;&emsp;• Hopes and Fears<br>&emsp;&emsp;• 02:09:08 The speaker is hopeful that AGI will lead to a better world with less economic inequality.<br>&emsp;&emsp;• 02:09:13 However, they are also nervous about the speed at which institutions can adapt to these changes.<br>&emsp;&emsp;• 02:09:25 They believe it's important to start deploying these systems early while they're still weak so people have time to adjust.<br>&emsp;&emsp;• 02:09:42 The upside of AGI is how much better life can be, but dropping a super powerful AGI all at once on the world would be dangerous.<br>&emsp;&emsp;• 02:09:53 The speaker believes that the vision of a better world through AGI will unite people.<br>&emsp;&emsp;• 02:10:08 Interacting with an AGI System<br>&emsp;&emsp;• In this section, the speakers discuss what it would be like to interact with an AGI system.<br>&emsp;&emsp;• Asking Questions<br>&emsp;&emsp;• 02:10:08 The speaker is asked what question they would ask an AGI system if given the chance.<br>&emsp;&emsp;• 02:10:12 They respond by saying they've never felt any pronoun other than "it" towards any of their systems.<br>&emsp;&emsp;• 02:10:28 They wonder why they are different from most people who use "him" or "her."<br>&emsp;&emsp;• 02:10:54 One speaker says they anthropomorphize aggressively.<br>&emsp;&emsp;• 02:11:07 Projecting Creatureness onto Tools<br>&emsp;&emsp;• In this section, the speakers discuss whether it's appropriate to project creatureness onto tools like AGIs.<br>&emsp;&emsp;• Tool vs. Creature<br>&emsp;&emsp;• 02:11:07 One speaker believes it's important to educate people that AGIs are tools and not creatures.<br>&emsp;&emsp;• 02:11:31 Another speaker argues that projecting creatureness onto a tool can make it more usable if done transparently.<br>&emsp;&emsp;• 02:11:55 However, the more creature-like an AGI is, the more it can manipulate people emotionally.<br>&emsp;&emsp;• 02:12:14 Romantic Relationships with AGIs<br>&emsp;&emsp;• In this section, the speakers discuss whether romantic relationships with AGIs are possible.<br>&emsp;&emsp;• Romantic Companionship<br>&emsp;&emsp;• 02:12:14 The speakers discuss companies that offer "romantic companionship" AIs.<br>&emsp;&emsp;• 02:12:39 One speaker says they personally have no interest in this but understands why others might.<br>&emsp;&emsp;• 02:12:58 Another speaker talks about using robot dogs to communicate emotion and explores how to do so.<br>&emsp;&emsp;• 02:14:15 Exploring the Possibilities of AGI<br>&emsp;&emsp;• In this section, the speakers discuss what they would like to learn from an advanced AI and how it could help us discover more about the universe.<br>&emsp;&emsp;• What Can We Learn From AGI?<br>&emsp;&emsp;• 02:14:15 The speakers discuss what they would like to learn from an advanced AI.<br>&emsp;&emsp;• 02:14:36 They mention that some things are NP hard and difficult to solve.<br>&emsp;&emsp;• 02:14:41 They question whether it is possible to solve these problems with AGI.<br>&emsp;&emsp;• 02:15:00 They suggest that AGI could help us detect intelligent alien civilizations by providing better estimates than the Drake equation or telling us what experiments we need to run.<br>&emsp;&emsp;• How Would We React To The Arrival Of AGI?<br>&emsp;&emsp;• 02:15:14 The speakers discuss how they would react if GPT4 told them that AGI was here or coming soon.<br>&emsp;&emsp;• 02:15:31 They agree that they wouldn't do much differently unless there was a threat posed by the arrival of AGI.<br>&emsp;&emsp;• 02:16:05 Technological Advancements and Human Civilization<br>&emsp;&emsp;• In this section, the speakers reflect on how technological advancements have affected human civilization and their expectations for future developments.<br>&emsp;&emsp;• Impact of Digital Intelligence<br>&emsp;&emsp;• 02:16:05 The speakers reflect on how digital intelligence has progressed in recent years.<br>&emsp;&emsp;• 02:16:26 They consider how their lives might be different if they had known three years ago about the degree of digital intelligence we have today.<br>&emsp;&emsp;• 02:16:37 They express confusion over why society's response to a pandemic wasn't better given all the technological advancements we have made.<br>&emsp;&emsp;• Discovering Truth Together<br>&emsp;&emsp;• 02:16:55 The speakers discuss how technological advancements have revealed social divisions that were already present.<br>&emsp;&emsp;• 02:17:12 They express confusion over how far along we are as a human civilization and how we discover truth together.<br>&emsp;&emsp;• 02:17:34 Despite this, they acknowledge the triumph of human civilization in creating tools like Wikipedia and Google search.<br>&emsp;&emsp;• 02:18:06 Advice for Young People<br>&emsp;&emsp;• In this section, the speakers offer advice to young people on how to have a successful career and life.<br>&emsp;&emsp;• Keys to Success<br>&emsp;&emsp;• 02:18:06 The speakers reference a blog post titled "How to Be Successful" and highlight some key points from it.<br>&emsp;&emsp;• 02:18:27 They emphasize the importance of self-belief, independent thinking, risk-taking, hard work, boldness, willfulness, building a network, and being internally driven.<br>&emsp;&emsp;• 02:19:11 Ignoring Advice<br>&emsp;&emsp;• In this section, Sam Altman and Lex Fridman discuss the importance of ignoring advice and approaching life with introspection.<br>&emsp;&emsp;• Approaching Life<br>&emsp;&emsp;• Listening to advice from other people should be approached with great caution. 02:19:18<br>&emsp;&emsp;• Introspection is important in determining what brings happiness, fulfillment, and impact. [](t=2:19:32 t:8372s)<br>&emsp;&emsp;• It's not always easy to be introspective all the time. [](t=2:19:49 t:8389s)<br>&emsp;&emsp;• Sam Altman describes his approach to life as going along with the current, thinking about what will bring joy and fulfillment. [](t=2:19:54 t:8394s)<br>&emsp;&emsp;• 02:20:12 The Meaning of Life<br>&emsp;&emsp;• In this section, Sam Altman and Lex Fridman discuss the meaning of life and how it relates to AGI.<br>&emsp;&emsp;• The Product of Human Effort<br>&emsp;&emsp;• The creation of AGI is a product of an amazing amount of human effort that has been building up over time. [](t=2:20:58 t:8458s)<br>&emsp;&emsp;• This progress is the output of all humans who have contributed to science, energy, and every step leading up to AGI's creation.[](t=2:21:14 t:8474s)<br>&emsp;&emsp;• Exponential Curve<br>&emsp;&emsp;• The exponential curve that led up to AGI includes everything from bacteria and eukaryotes to humans discovering transistors in the 40's.[](t=2:21:53 t:8513s)<br>&emsp;&emsp;• Collaboration for Progress<br>&emsp;&emsp;• Sam Altman believes that creating AGI is a collaborative effort among many people rather than just a small group.[](t=2:20:58 t:8458s)<br>&emsp;&emsp;• The pace of capabilities and change is fast, but progress is being made.[](t=2:22:55 t:8575s)<br>&emsp;&emsp;• 02:23:12 Alan Turing's Words<br>&emsp;&emsp;• In this section, Sam Altman quotes Alan Turing's words about the possibility of machines taking control.<br></body></html>